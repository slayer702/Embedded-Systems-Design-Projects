#!/usr/bin/env python         
# -*- coding: utf-8 -*-

# USAGE
# python detect_realtime_tinyyolo_ncs.py --conf config/config.json \
# 	--input videos/test_video.mp4



# import the necessary packages for OpenCV and TinyYOLOv3
from openvino.inference_engine import IENetwork
from openvino.inference_engine import IEPlugin
from intel.yoloparams import TinyYOLOV3Params
from intel.tinyyolo import TinyYOLOv3
from imutils.video import VideoStream
from pyimagesearch.utils import Conf
from imutils.video import FPS
import numpy as np
import argparse
import imutils
import time
import cv2
import os

#------Sensor Imports------
#from JY901 import *
import time
import serial
import threading
#from multiprocessing import Process
import multiprocessing

#------Setup Serial Ports------
ser = serial.Serial("/dev/serial/by-path/platform-3f980000.usb-usb-0:1.1.2.1:1.0-port0", 115200)     #Left
ser2 = serial.Serial("/dev/serial/by-path/platform-3f980000.usb-usb-0:1.1.2.2:1.0-port0", 115200)    #Right
portname = "/dev/serial/by-path/platform-3f980000.usb-usb-0:1.2:1.0-port0"                       #IMU


#------Detection Variables------
#t1 = 0
#persondetect = False
#cardetect = False

'''
#------Initialize WT931#------
print ("Initializing WT931 Code...")
baudrate = 9600
wt931 = CJY901()

try:
    serialPort = serial.Serial(portname, baudrate)
    serialPort.writeTimeout = 2 #写超时 Write timeout
    print ("Serial port opened successfully%s"%portname),
    print (",Baud rate%s"%baudrate)
except Exception as e:
    print ("The serial port is occupied, please restart")

def ImuFunc():
    while True:
            if serialPort.inWaiting() >= 44:
                #print("time cost%s"%(time.time() - t0))
                t0 = time.time()
                #print("Number of serial buffers %s"%serialPort.inWaiting())
                try:
                    rxBuffer = list(serialPort.read(44))
                    #print(rxBuffer)
                    for i in range(0, 44):
                        wt931.CopeSerialData(rxBuffer[i])
                except Exception as e:
                    print("Clean up the serial port"),
                    serialPort.flush()
                    time.sleep(5)
                    print("Clean up the serial port")
            else:
                pass
            #time.sleep(0.0005)
            #time.sleep(0.250)
'''


'''
#------Initialize OpenCv and TinyYOLOv3------

# construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--conf", required=True,
    help="Path to the input configuration file")
ap.add_argument("-i", "--input", help="path to the input video file")
args = vars(ap.parse_args())

# load the configuration file
conf = Conf(args["conf"])

# load the COCO class labels our YOLO model was trained on and
# initialize a list of colors to represent each possible class
# label
LABELS = open(conf["labels_path"]).read().strip().split("\n")
np.random.seed(42)
COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))

# initialize the plugin in for specified device
#plugin = IEPlugin(device="MYRIAD")
plugin = IEPlugin(device="MYRIAD")

# read the IR generated by the Model Optimizer (.xml and .bin files)
print("[INFO] loading models...")
net = IENetwork(model=conf["xml_path"], weights=conf["bin_path"])

# prepare inputs
print("[INFO] preparing inputs...")
inputBlob = next(iter(net.inputs))

# set the default batch size as 1 and get the number of input blobs,
# number of channels, the height, and width of the input blob
net.batch_size = 1
(n, c, h, w) = net.inputs[inputBlob].shape

# if a video path was not supplied, grab a reference to the webcam
if args["input"] is None:
    print("[INFO] starting video stream...")
    vs = VideoStream(src=0).start()
    
    #vs = VideoStream(usePiCamera=True).start()
    time.sleep(2.0)

# otherwise, grab a reference to the video file
else:
    print("[INFO] opening video file...")
    vs = cv2.VideoCapture(os.path.abspath(args["input"]))

# loading model to the plugin and start the frames per second
# throughput estimator
print("[INFO] loading model to the plugin...")
execNet = plugin.load(network=net, num_requests=1)
fps = FPS().start()
'''


print("[INFO] loading TFmini Plus Threads")
'''
thread1.start()
thread2.start()
thread1.join()
thread2.join()
'''
'''
if ser.is_open == False:
    ser.open()
    ser2.open()
#getTFminiData()
#getTFminiData2()
thread1.start()
thread2.start()
thread1.join()
thread2.join()
'''



#------Initialize TFmini Plus------
print ("Initializing Lidar Code...")

#def getTFminiData():
def getTFminiData():
    global distance
    while True:
        #time.sleep(0.1)
        count = ser.in_waiting
        if count > 8:
            recv = ser.read(9)  
            ser.reset_input_buffer()  
            
            if recv[0] == 0x59 and recv[1] == 0x59:     #python3
                with lock:
                    distance.value = recv[2] + recv[3] * 256
                    #strength = recv[4] + recv[5] * 256
                    #print('(', distance, ',', strength, ')')
                    print('Left Lidar: ', distance.value)
                    ser.reset_input_buffer()
                    #time.sleep(0.0005)
                    time.sleep(0.333)

                
def getTFminiData2():
    global distance2
    while True:
        #time.sleep(0.1)
        count2 = ser2.in_waiting
        if count2 > 8:
            recv2 = ser2.read(9)  
            ser2.reset_input_buffer()  
            
            if recv2[0] == 0x59 and recv2[1] == 0x59:     #python3
                with lock:
                    distance2.value = recv2[2] + recv2[3] * 256
                    #strength2 = recv2[4] + recv2[5] * 256
                    #print('(', distance2, ',', strength2, ')')
                    print('Right Lidar: ', distance2.value)
                    ser2.reset_input_buffer()
                    #time.sleep(0.0005)
                    time.sleep(0.333)

#thread1 = threading.Thread(target=getTFminiData)
#thread2 = threading.Thread(target=getTFminiData2)




def objectDetect():
    #------Initialize OpenCv and TinyYOLOv3------
    
    #Initialize Variables
    t1 = 0
    
    DIM = (640,480)
    K = np.array([[533.5429584636792697, 0.0, 317.8786110037330559], [0.0, 532.7336812359551459, 230.3619256340880668], [0.0, 0.0, 1.0]])
    D = np.array([[-1.619588998699278415], [2.279079664120869708],  [-0.002046634026002930578], [0.005608371892839346742]])

    balance = 1.0;
    dim2=None
    dim3=None
    
    # construct the argument parser and parse the arguments
    ap = argparse.ArgumentParser()
    ap.add_argument("-c", "--conf", required=True,
        help="Path to the input configuration file")
    ap.add_argument("-i", "--input", help="path to the input video file")
    args = vars(ap.parse_args())

    # load the configuration file
    conf = Conf(args["conf"])

    # load the COCO class labels our YOLO model was trained on and
    # initialize a list of colors to represent each possible class
    # label
    LABELS = open(conf["labels_path"]).read().strip().split("\n")
    np.random.seed(42)
    COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))

    # initialize the plugin in for specified device
    #plugin = IEPlugin(device="MYRIAD")
    plugin = IEPlugin(device="MYRIAD")

    # read the IR generated by the Model Optimizer (.xml and .bin files)
    print("[INFO] loading models...")
    net = IENetwork(model=conf["xml_path"], weights=conf["bin_path"])

    # prepare inputs
    print("[INFO] preparing inputs...")
    inputBlob = next(iter(net.inputs))

    # set the default batch size as 1 and get the number of input blobs,
    # number of channels, the height, and width of the input blob
    net.batch_size = 1
    (n, c, h, w) = net.inputs[inputBlob].shape

    # if a video path was not supplied, grab a reference to the webcam
    if args["input"] is None:
        print("[INFO] starting video stream...")
        vs = VideoStream(src=0).start()
        
        #vs = VideoStream(usePiCamera=True).start()
        time.sleep(2.0)

    # otherwise, grab a reference to the video file
    else:
        print("[INFO] opening video file...")
        vs = cv2.VideoCapture(os.path.abspath(args["input"]))

    # loading model to the plugin and start the frames per second
    # throughput estimator
    print("[INFO] loading model to the plugin...")
    execNet = plugin.load(network=net, num_requests=1)
    fps = FPS().start()
    # loop over the frames from the video stream
    while True:

        #Read IMU Data
        #ImuFunc()

        # grab the next frame and handle if we are reading from either
        # VideoCapture or VideoStream
        orig = vs.read()
        orig = orig[1] if args["input"] is not None else orig

        # if we are viewing a video and we did not grab a frame then we
        # have reached the end of the video
        if args["input"] is not None and orig is None:
            break

        # resize original frame to have a maximum width of 500 pixel and
        # input_frame to network size
        orig = imutils.resize(orig, width=500)
        
        
        
        
                #flag, imgold = cap.read()
        img = cv2.resize(orig, (640,480))
        
        #img = cap.read()
        dim1 = img.shape[:2][::-1]  #dim1 is the dimension of input image to un-distort
        assert dim1[0]/dim1[1] == DIM[0]/DIM[1], "Image to undistort needs to have same aspect ratio as the ones used in calibration"
        if not dim2:
            dim2 = dim1
        if not dim3:
            dim3 = dim1
        scaled_K = K * dim1[0] / DIM[0]  # The values of K is to scale with image dimension.
        scaled_K[2][2] = 1.0  # Except that K[2][2] is always 1.0
        # This is how scaled_K, dim2 and balance are used to determine the final K used to un-distort image. OpenCV document failed to make this clear!
        new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, D, dim2, np.eye(3), balance=balance)


        map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, D, np.eye(3), new_K, dim3, cv2.CV_16SC2)
    
        #map1, map2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), K, DIM, cv2.CV_16SC2)
        calibrated = cv2.remap(img, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)
        #cv2.imshow('result', calibrated)
        
        
        
        
        
        
        
        
        
        
        #frame = cv2.resize(orig, (w, h))
        frame = cv2.resize(calibrated, (w, h))

        # change data layout from HxWxC to CxHxW
        frame = frame.transpose((2, 0, 1))
        frame = frame.reshape((n, c, h, w))











        # start inference and initialize list to collect object detection
        # results
        output = execNet.infer({inputBlob: frame})
        objects = []

        # loop over the output items
        for (layerName, outBlob) in output.items():
            # create a new object which contains the required tinyYOLOv3
            # parameters
            layerParams = TinyYOLOV3Params(net.layers[layerName].params,
                outBlob.shape[2])

            # parse the output region
            objects += TinyYOLOv3.parse_yolo_region(outBlob,
                #frame.shape[2:], orig.shape[:-1], layerParams,
                frame.shape[2:], calibrated.shape[:-1], layerParams,
                conf["prob_threshold"])

        # loop over each of the objects
        for i in range(len(objects)):
            # check if the confidence of the detected object is zero, if
            # it is, then skip this iteration, indicating that the object
            # should be ignored
            if objects[i]["confidence"] == 0:
                continue

            # loop over remaining objects
            for j in range(i + 1, len(objects)):
                # check if the IoU of both the objects exceeds a
                # threshold, if it does, then set the confidence of that
                # object to zero
                if TinyYOLOv3.intersection_over_union(objects[i],
                    objects[j]) > conf["iou_threshold"]:
                    objects[j]["confidence"] = 0

        # filter objects by using the probability threshold -- if a an
        # object is below the threshold, ignore it
        objects = [obj for obj in objects if obj['confidence'] >= \
            conf["prob_threshold"]]

        # store the height and width of the original frame
        #(endY, endX) = orig.shape[:-1]
        (endY, endX) = calibrated.shape[:-1]

        # loop through all the remaining objects
        #for obj in objects:
        for obj in objects:
            # validate the bounding box of the detected object, ensuring
            # we don't have any invalid bounding boxes
            if obj["xmax"] > endX or obj["ymax"] > endY or obj["xmin"] < 0 or obj["ymin"] < 0:
                continue

            # build a label consisting of the predicted class and
            # associated probability
            label = "{}: {:.2f}%".format(LABELS[obj["class_id"]], obj["confidence"] * 100)
            label_detect = (LABELS[obj["class_id"]])
            
            detectermans = str(label_detect)
            
            print(detectermans)
            # calculate the y-coordinate used to write the label on the
            # frame depending on the bounding box coordinate
            y = obj["ymin"] - 15 if obj["ymin"] - 15 > 15 else obj["ymin"] + 15

            # draw a bounding box rectangle and label on the frame
            #if detectermans == check:
            if detectermans == "car":
                #cardetect = True
                #cv2.rectangle(orig, (obj["xmin"], obj["ymin"]), (obj["xmax"], obj["ymax"]), COLORS[obj["class_id"]], 2)
                cv2.rectangle(calibrated, (obj["xmin"], obj["ymin"]), (obj["xmax"], obj["ymax"]), COLORS[obj["class_id"]], 2)
                #cv2.putText(orig, label, (obj["xmin"], y), cv2.FONT_HERSHEY_SIMPLEX, 1, COLORS[obj["class_id"]], 3)
                cv2.putText(calibrated, label, (obj["xmin"], y), cv2.FONT_HERSHEY_SIMPLEX, 1, COLORS[obj["class_id"]], 3)
                #cv2.putText(orig, "Car Detected: [X]", (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Car Display
                cv2.putText(calibrated, "Car Detected: [X]", (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Car Display
            #else:
            #	cardetect = False

            if detectermans == "person":
                #persondetect = True
                #cv2.rectangle(orig, (obj["xmin"], obj["ymin"]), (obj["xmax"], obj["ymax"]), COLORS[obj["class_id"]], 2)
                cv2.rectangle(calibrated, (obj["xmin"], obj["ymin"]), (obj["xmax"], obj["ymax"]), COLORS[obj["class_id"]], 2)
                #cv2.putText(orig, label, (obj["xmin"], y), cv2.FONT_HERSHEY_SIMPLEX, 1, COLORS[obj["class_id"]], 3)
                cv2.putText(calibrated, label, (obj["xmin"], y), cv2.FONT_HERSHEY_SIMPLEX, 1, COLORS[obj["class_id"]], 3)
                #cv2.putText(orig, "Person Detected: [X]", (15, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Person Display
                cv2.putText(calibrated, "Person Detected: [X]", (15, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Person Display
            #else:
            #	persondetect = False


        elapsedTime = time.time() - t1
        framespersec = "{:.1f} FPS".format(1/elapsedTime)
        distancedisplay = "Left Lidar: {:.1f}".format(distance.value)
        distancedisplay2 = "Right Lidar: {:.1f}".format(distance2.value)

        #cv2.putText(orig, (framespersec), (15, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Fps Display
        cv2.putText(calibrated, (framespersec), (15, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Fps Display
        
        #cv2.putText(orig, (distancedisplay), (15, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Left Lidar Data Display
        cv2.putText(calibrated, (distancedisplay), (15, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Left Lidar Data Display
        #cv2.putText(orig, (distancedisplay2), (15, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Right Lidar Data Display
        cv2.putText(calibrated, (distancedisplay2), (15, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 1, cv2.LINE_AA) #Right Lidar Data Display
        


        # display the current frame to the screen and record if a user
        # presses a key
        #cv2.imshow("Interactive Helmet - Blindspot Detector", orig)
        cv2.imshow("Interactive Helmet - Blindspot Detector", calibrated)
        key = cv2.waitKey(1) & 0xFF



        # if the `q` key was pressed, break from the loop
        if key == ord("q"):
            #ser.close()
            #ser2.close()
            # stop the timer and display FPS information
            fps.stop()
            print("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))
            print("[INFO] approx. FPS: {:.2f}".format(fps.fps()))
            print("n: ", n)
            print("c: ", c)
            print("h: ", h)
            print("w: ", w)
            # stop the video stream and close any open windows1
            vs.stop() if args["input"] is None else vs.release()
            cv2.destroyAllWindows()
            #break
            
        t1 = time.time()

        # update the FPS counter
        fps.update()



distance = multiprocessing.Manager().Value('i', 0)
distance2 = multiprocessing.Manager().Value('i', 0)
lock = multiprocessing.Manager().Lock()

'''
thread1 = Process(target=getTFminiData, args=(distance,))
thread2 = Process(target=getTFminiData2)
thread3 = Process(target=objectDetect, args=(distance,))
'''
thread1 = multiprocessing.Process(target=getTFminiData)
thread2 = multiprocessing.Process(target=getTFminiData2)
thread3 = multiprocessing.Process(target=objectDetect)
thread1.start()
thread2.start()
thread3.start()
thread1.join()
thread2.join()
thread3.join()


print("Starting Process 1 and 3")


'''
# stop the timer and display FPS information
fps.stop()
print("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))
print("[INFO] approx. FPS: {:.2f}".format(fps.fps()))
print("n: ", n)
print("c: ", c)
print("h: ", h)
print("w: ", w)
# stop the video stream and close any open windows1
vs.stop() if args["input"] is None else vs.release()
cv2.destroyAllWindows()
'''